{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze and Visualize Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Specify the path to the data folder\n",
    "data_folder = '..data/'  # Adjust this if your folder structure is different\n",
    "\n",
    "# Use glob to get all CSV files in the data folder\n",
    "csv_files = glob.glob(os.path.join(data_folder, '*.csv'))\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the list of files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Optionally, concatenate all DataFrames into a single DataFrame\n",
    "combined_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Now you can work with combined_data\n",
    "print(combined_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Key Indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data['date'], data['SMA'], label='SMA', color='blue')\n",
    "plt.plot(data['date'], data['EMA'], label='EMA', color='orange')\n",
    "plt.legend()\n",
    "plt.title('SMA and EMA Trends')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyNance for Financial Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynance as pn\n",
    "\n",
    "def calculate_volatility(df):\n",
    "    df['volatility'] = df['close'].rolling(window=10).std()\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv('task_2/data/indicators_data.csv')\n",
    "df_with_volatility = calculate_volatility(df)\n",
    "df_with_volatility.to_csv('task_2/data/final_data.csv', index=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (1226969161.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    all_files = [f for f in os.listdir(data_folder) if f.endswith('.csv')]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_and_clean_data(data_folder):\n",
    "    all_files = [f for f in os.listdir(data_folder) if f.endswith('.csv')]\n",
    "    df_list = [pd.read_csv(os.path.join(data_folder, file)) for file in all_files]\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'], errors='coerce')\n",
    "    return combined_df.dropna()\n",
    "\n",
    "data = load_and_clean_data('week_1/data')\n",
    "data.to_csv('..data/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1. Run Sentiment Analysis: Execute scripts/sentiment_analysis.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1945954782.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python scripts/sentiment_analysis.py\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python scripts/sentiment_analysis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2. Calculate Returns: Execute scripts/calculate_returns.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1207841381.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python scripts/calculate_returns.py\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python scripts/calculate_returns.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3. Analyze Correlation: Execute scripts/correlation_analysis.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "python scripts/correlation_analysis.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4. Visualize Results: Execute scripts/visualization.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python scripts/visualization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import talib\n",
    "from textblob import TextBlob\n",
    "\n",
    "def load_and_clean_data(data_folder):\n",
    "    \"\"\"\n",
    "    Load all CSV files from a folder, combine them, and clean the data.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in os.listdir(data_folder) if f.endswith('.csv')]\n",
    "    if not all_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {data_folder}\")\n",
    "    \n",
    "    # Combine all data files\n",
    "    df_list = [pd.read_csv(os.path.join(data_folder, file)) for file in all_files]\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Ensure 'date' column is in datetime format and drop nulls\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'], errors='coerce')\n",
    "    combined_df = combined_df.dropna(subset=['date', 'close'])  # Remove rows with invalid dates or prices\n",
    "    \n",
    "    print(\"Data loaded and cleaned successfully.\")\n",
    "    return combined_df\n",
    "\n",
    "def add_sentiment_analysis(data):\n",
    "    \"\"\"\n",
    "    Add sentiment scores to the dataset if 'headline' column exists.\n",
    "    \"\"\"\n",
    "    if 'headline' in data.columns:\n",
    "        data['sentiment'] = data['headline'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "        print(\"Sentiment analysis added.\")\n",
    "    else:\n",
    "        print(\"No 'headline' column found. Skipping sentiment analysis.\")\n",
    "    return data\n",
    "\n",
    "def calculate_returns(data):\n",
    "    \"\"\"\n",
    "    Calculate daily percentage returns for the 'close' column.\n",
    "    \"\"\"\n",
    "    data['returns'] = data['close'].pct_change()\n",
    "    print(\"Daily returns calculated.\")\n",
    "    return data\n",
    "\n",
    "def add_technical_indicators(data):\n",
    "    \"\"\"\n",
    "    Add common technical indicators using TA-Lib.\n",
    "    \"\"\"\n",
    "    data['SMA'] = talib.SMA(data['close'], timeperiod=14)\n",
    "    data['EMA'] = talib.EMA(data['close'], timeperiod=14)\n",
    "    data['RSI'] = talib.RSI(data['close'], timeperiod=14)\n",
    "    data['MACD'], data['MACD_signal'], _ = talib.MACD(data['close'])\n",
    "    print(\"Technical indicators calculated.\")\n",
    "    return data\n",
    "\n",
    "def analyze_correlation(data, output_path):\n",
    "    \"\"\"\n",
    "    Analyze correlations between numeric columns and save the results.\n",
    "    \"\"\"\n",
    "    correlation_matrix = data.corr()\n",
    "    correlation_matrix.to_csv(output_path)\n",
    "    print(f\"Correlation analysis saved to {output_path}\")\n",
    "\n",
    "def save_data(data, output_path):\n",
    "    \"\"\"\n",
    "    Save the final dataset to a CSV file.\n",
    "    \"\"\"\n",
    "    data.to_csv(output_path, index=False)\n",
    "    print(f\"Final data saved to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    data_folder = \"week-1/data\"\n",
    "    correlation_output_path = \"week-1/correlation_matrix.csv\"\n",
    "    final_output_path = \"week-1/final_results.csv\"\n",
    "\n",
    "    # Load and clean data\n",
    "    data = load_and_clean_data(data_folder)\n",
    "    \n",
    "    # Perform sentiment analysis\n",
    "    data = add_sentiment_analysis(data)\n",
    "    \n",
    "    # Calculate returns\n",
    "    data = calculate_returns(data)\n",
    "    \n",
    "    # Add technical indicators\n",
    "    data = add_technical_indicators(data)\n",
    "    \n",
    "    # Analyze correlations\n",
    "    analyze_correlation(data, correlation_output_path)\n",
    "    \n",
    "    # Save the final dataset\n",
    "    save_data(data, final_output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
